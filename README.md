# GitHub Issue Summarizer

![Python](https://img.shields.io/badge/python-3.8%2B-blue)
![License](https://img.shields.io/github/license/‰Ω†ÁöÑÁî®Êà∑Âêç/‰Ω†ÁöÑ‰ªìÂ∫ìÂêç)
![Stars](https://img.shields.io/github/stars/MoeMoeCS/LLM-issue?style=social)

[ÁÆÄ‰Ωì‰∏≠Êñá](README.zh.md) | English

A CLI tool to fetch, filter, and summarize GitHub issues, with one-line summaries generated by LLMs (e.g., OpenAI).

---

## üöÄ Quick Start (Zero Hassle)

1. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```
2. **Run with one command:**
   ```bash
   python llm-issue.py owner/repo --max-issues 30
   ```
   - Replace `owner/repo` with the target GitHub repository, e.g. `AstrBotDevs/AstrBot`.
   - `--max-issues` controls how many open issues to fetch (default 50, recommended for large repos).
   - **No need to manually export environment variables!**
   - If `GH_TOKEN` or `OPENAI_API_KEY` is missing, you will be prompted to enter them once, and they will be saved to `.env` for future runs.

3. **Results:**
   - Output files are saved to the `output/` directory automatically.
   - Includes a Markdown summary and filtered issues in JSON.

---

## Features

- Fetches open issues from any GitHub repository
- Filters out assigned, closed, or noisy issues
- Classifies issues by type and priority
- Generates one-line summaries for each issue using LLM (e.g., OpenAI GPT)
- Outputs a Markdown table and project overview
- Caches API responses and summaries for better performance
- Supports batch processing with automatic rate limiting
- Quality control for generated summaries
- Fallback to local summarization when LLM is unavailable

## Advanced Usage

### Optional Arguments

- `--max-issues N`  Limit the number of open issues to fetch (default 50). Useful for large repositories to speed up the process.

### Optional Environment Variables

You can still use environment variables or `.env` to customize behavior:

```bash
# Caching options
CACHE_DB_PATH=.cache/cache.db         # Cache database location
CACHE_MAX_MEMORY_ITEMS=1000           # Max items in memory cache
CACHE_CLEANUP_INTERVAL=3600           # Cache cleanup interval (seconds)

# LLM options
LLM_CONCURRENCY_LIMIT=10              # Max concurrent LLM requests
OPENAI_BASE_URL=https://api.openai.com/v1  # API endpoint
MODEL_NAME=gpt-3.5-turbo              # Model to use
```

### Configuration

- Edit `config.py` to customize:
  - Keywords for issue classification
  - Priority rules and labels
  - LLM prompt templates
  - Filter rules for issues

## Output Format

The tool generates:
1. A project overview with issue statistics
2. A detailed Markdown table with:
   - Issue number and type
   - Priority level
   - One-line summary
   - Key labels
   - Creation date
   - Direct link

Example output:
```markdown
# owner/repo Issues Overview

Currently has **50** open issues (20 bugs / 15 feature requests), average priority P1, last updated on 2024-03-20.

| #Issue | Type | Priority | Title | Summary | Key Labels | Created | Link |
|--------|------|----------|-------|---------|------------|---------|------|
| #123 | Bug | P1 | Login fails | "Login page error under high concurrency" | backend, critical | 2024-03-19 | üîó |
```

## Dependencies

- Python 3.8+
- httpx
- typer
- pydantic
- rich
- openai
- python-dotenv

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## License

Apache License 2.0
