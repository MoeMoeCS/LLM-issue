#!/usr/bin/env python3
"""
GitHub Issue é€Ÿè§ˆå™¨ CLI
æ”¯æŒå¤§æ¨¡å‹è‡ªåŠ¨ç”Ÿæˆä¸€å¥è¯æ‘˜è¦
"""
from __future__ import annotations

import asyncio
import json
import re
import sys
from datetime import datetime
from pathlib import Path
from typing import List, Optional, Tuple

import httpx
import typer
from pydantic import BaseModel
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn
from rich.table import Table

from cache_keys import get_github_issues_key
from config import (
    DONE_KEYWORDS,
    NOISE_LABELS,
    PRIORITY_RULES,
    PRIORITY_STRINGS,
    TYPE_PATTERNS,
)
from exceptions import GitHubError, RateLimitError, RepoNotFoundError, TokenError, NetworkError
from llm_summary import summarize_batch
from utils import setup_logger
from cache import get_cache, set_cache

logger = setup_logger()

# ---------- å¸¸é‡ ----------
GITHUB_API = "https://api.github.com"
PER_PAGE = 100
MAX_ITEMS = 10_000
MAX_RETRIES = 3  # æœ€å¤§é‡è¯•æ¬¡æ•°

# ---------- æ•°æ®æ¨¡å‹ ----------
class Issue(BaseModel):
    """å•ä¸ª GitHub Issue çš„ç»“æ„åŒ–ä¿¡æ¯"""

    number: int
    title: str
    body: str | None
    labels: List[str]
    assignees: List[str]
    state: str
    created_at: datetime
    updated_at: datetime
    html_url: str
    type_: str = ""
    priority: str = ""

    def to_dict(self) -> dict:
        """å…¼å®¹ä¸åŒç‰ˆæœ¬çš„Pydanticï¼Œå°†Issueå¯¹è±¡è½¬æ¢ä¸ºå­—å…¸"""
        try:
            # å°è¯•ä½¿ç”¨ v2 æ–¹æ³•
            return self.model_dump(mode="json")
        except AttributeError:
            try:
                # å°è¯•ä½¿ç”¨ v1 æ–¹æ³•
                return self.dict()
            except AttributeError:
                # å¦‚æœéƒ½å¤±è´¥ï¼Œæ‰‹åŠ¨è½¬æ¢
                return {
                    "number": self.number,
                    "title": self.title,
                    "body": self.body,
                    "labels": self.labels,
                    "assignees": self.assignees,
                    "state": self.state,
                    "created_at": self.created_at.isoformat(),
                    "updated_at": self.updated_at.isoformat(),
                    "html_url": self.html_url,
                    "type_": self.type_,
                    "priority": self.priority,
                }


# ---------- åˆ†ç±»ä¸è¿‡æ»¤ ----------
def classify_issue(issue: Issue) -> Issue:
    """æ ¹æ®æ ‡é¢˜å’Œæ­£æ–‡æ¨æ–­ issue çš„ç±»å‹ä¸ä¼˜å…ˆçº§"""
    text = f"{issue.title} {issue.body or ''}".lower()

    # ç±»å‹
    for issue_type, patterns in TYPE_PATTERNS.items():
        if any(re.search(p, text, re.I) for p in patterns):
            issue.type_ = issue_type
            break
    else:
        issue.type_ = "Other"

    # ä¼˜å…ˆçº§
    for prio, patterns in PRIORITY_RULES.items():
        if any(re.search(p, text, re.I) for p in patterns):
            issue.priority = prio
            break
        # é¢å¤–æ£€æŸ¥ label é‡Œæ˜¯å¦ç›´æ¥åŒ…å«å­—ç¬¦ä¸²
        if any(s.strip("/") in issue.labels for s in PRIORITY_STRINGS[prio]):
            issue.priority = prio
            break
    else:
        issue.priority = "P2"
    return issue


def should_include(issue: Issue) -> bool:
    """è¿”å› True è¡¨ç¤ºä¿ç•™è¯¥ issue"""
    if issue.state != "open":
        return False
    if issue.assignees:
        return False
    content = f"{issue.title} {issue.body or ''}".lower()
    if any(kw in content for kw in DONE_KEYWORDS):
        return False
    if any(lbl.lower() in map(str.lower, NOISE_LABELS) for lbl in issue.labels):
        return False
    return True


# ---------- GitHub API ç›¸å…³ ----------
async def _handle_github_response(
    r: httpx.Response,
    repo: str,
) -> Tuple[bool, Optional[List[dict]]]:
    """å¤„ç† GitHub API å“åº”ï¼Œè¿”å› (æ˜¯å¦ç»§ç»­, æ•°æ®)"""
    if r.status_code == 404:
        raise RepoNotFoundError(f"Repository {repo} not found")
    
    if r.status_code == 403:
        # æ£€æŸ¥æ˜¯å¦æ˜¯ rate limit
        if "rate limit exceeded" in r.text.lower():
            reset_ts = int(r.headers.get("x-ratelimit-reset", 0))
            raise RateLimitError(reset_ts)
        # å…¶ä»– 403 é”™è¯¯ï¼ˆå¦‚ token æ— æ•ˆï¼‰
        raise TokenError(f"Token error or permission denied: {r.text}")
    
    if r.status_code == 422:
        logger.info("No more issues (422), stopping.")
        return False, None
    
    # å…¶ä»–é”™è¯¯
    r.raise_for_status()
    data = r.json()
    if not data:
        return False, None
    
    return True, data

async def fetch_issues(repo: str, token: str | None) -> List[Issue]:
    """æŠ“å–æŒ‡å®šä»“åº“çš„ open issuesï¼Œè‡ªåŠ¨åœäº GitHub ä¸Šé™"""
    headers = {"Accept": "application/vnd.github+json"}
    if token:
        headers["Authorization"] = f"Bearer {token}"

    issues: List[Issue] = []
    page = 1

    # ç”Ÿæˆç¼“å­˜é”®
    cache_key = get_github_issues_key(repo, token)
    cached_data = get_cache(cache_key)
    if cached_data:
        logger.info("Using cached issues data")
        return [Issue(**issue_dict) for issue_dict in cached_data]

    async with httpx.AsyncClient(
        limits=httpx.Limits(max_keepalive_connections=20, max_connections=100),
        timeout=30,
    ) as client:
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            transient=True,
            console=Console(stderr=True),
        ) as progress:
            task = progress.add_task("Fetching issues...", total=None)
            
            while len(issues) < MAX_ITEMS:
                for attempt in range(MAX_RETRIES):
                    try:
                        r = await client.get(
                            f"{GITHUB_API}/repos/{repo}/issues",
                            headers=headers,
                            params={
                                "state": "open",
                                "sort": "updated",
                                "direction": "desc",
                                "per_page": PER_PAGE,
                                "page": page,
                            },
                        )
                        
                        should_continue, data = await _handle_github_response(r, repo)
                        if not should_continue:
                            break
                        
                        for item in data:
                            if "pull_request" in item:
                                continue
                            issue = Issue(
                                number=item["number"],
                                title=item["title"],
                                body=item.get("body") or "",
                                labels=[l["name"] for l in item["labels"]],
                                assignees=[a["login"] for a in item["assignees"]],
                                state=item["state"],
                                created_at=datetime.fromisoformat(
                                    item["created_at"].replace("Z", "+00:00")
                                ),
                                updated_at=datetime.fromisoformat(
                                    item["updated_at"].replace("Z", "+00:00")
                                ),
                                html_url=item["html_url"],
                            )
                            issue = classify_issue(issue)
                            if should_include(issue):
                                issues.append(issue)
                        
                        # æ£€æŸ¥æ˜¯å¦éœ€è¦é™æµ
                        remain = int(r.headers.get("x-ratelimit-remaining", 1))
                        reset_ts = int(r.headers.get("x-ratelimit-reset", 0))
                        if remain < 10 and reset_ts:
                            wait = max(reset_ts - int(datetime.now().timestamp()), 0) + 1
                            logger.warning("Rate limit low (%d remaining), sleeping %ds", remain, wait)
                            await asyncio.sleep(wait)
                        
                        page += 1
                        progress.update(task, advance=PER_PAGE)
                        break  # æˆåŠŸè·å–æ•°æ®ï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                        
                    except RateLimitError as e:
                        if attempt == MAX_RETRIES - 1:
                            logger.error("Rate limit exceeded and max retries reached")
                            raise
                        wait = max(e.reset_time - int(datetime.now().timestamp()), 0) + 1
                        logger.warning("Rate limit exceeded, sleeping %ds (attempt %d/%d)", wait, attempt + 1, MAX_RETRIES)
                        await asyncio.sleep(wait)
                        
                    except (httpx.RequestError, httpx.HTTPError) as e:
                        if attempt == MAX_RETRIES - 1:
                            logger.error("Network error after %d retries: %s", MAX_RETRIES, e)
                            raise NetworkError(f"Failed to fetch issues: {e}")
                        wait = 2 ** attempt  # æŒ‡æ•°é€€é¿
                        logger.warning("Network error, retrying in %ds (attempt %d/%d): %s", wait, attempt + 1, MAX_RETRIES, e)
                        await asyncio.sleep(wait)
                else:
                    # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
                    logger.error("Failed to fetch issues after %d retries", MAX_RETRIES)
                    raise NetworkError("Failed to fetch issues after all retries")

    logger.info("Fetched %d issues after filtering", len(issues))
    
    # ç¼“å­˜ç»“æœï¼Œè®¾ç½® 5 åˆ†é’Ÿè¿‡æœŸæ—¶é—´
    issues_data = [issue.to_dict() for issue in issues]
    set_cache(cache_key, issues_data, expire_in=300)
    
    return issues


# ---------- æ‘˜è¦ & è¾“å‡º ----------
async def build_summary_async(issues: List[Issue], repo: str) -> tuple[str, str]:
    """ç”Ÿæˆé¡¹ç›®æ€»è§ˆä¸ Markdown è¡¨æ ¼"""
    total = len(issues)
    bugs = sum(1 for i in issues if i.type_ == "Bug")
    features = sum(1 for i in issues if i.type_ == "Feature Request")
    scores = {"P0": 0, "P1": 1, "P2": 2}
    avg_score = (
        round(sum(scores.get(i.priority, 2) for i in issues) / total) if total else 2
    )
    latest = (
        max(issues, key=lambda i: i.updated_at).updated_at.strftime("%Y-%m-%d")
        if issues
        else "N/A"
    )
    oneliner = (
        f"{repo} ç›®å‰å…±æœ‰ **{total}** ä¸ªå¾…è§£å†³ Issue"
        f"ï¼ˆBug {bugs} ä¸ª / æ–°åŠŸèƒ½ {features} ä¸ªï¼‰ï¼Œ"
        f"å¹³å‡ä¼˜å…ˆçº§ P{avg_score}ï¼Œæœ€æ–°æ›´æ–°äº {latest}ã€‚"
    )

    summaries = await summarize_batch(issues[:100])
    md_rows = [
        "| #Issue | ç±»å‹ | ä¼˜å…ˆçº§ | æ ‡é¢˜ | ä¸€å¥è¯æ‘˜è¦ | å…³é”®æ ‡ç­¾ | åˆ›å»ºæ—¶é—´ | åœ°å€ |",
        "| --- | --- | --- | --- | --- | --- | --- | --- |",
    ]
    for issue, summary in zip(issues[:100], summaries):
        labels = ", ".join(issue.labels[:3])
        md_rows.append(
            f"| {issue.number} | {issue.type_} | {issue.priority} | {issue.title} "
            f"| {summary} | {labels} | {issue.created_at.date()} "
            f"| [ğŸ”—]({issue.html_url}) |"
        )
    return oneliner, "\n".join(md_rows)


def save_outputs(repo: str, oneliner: str, md_table: str, issues: List[Issue]) -> None:
    """ä¿å­˜ç»“æœåˆ°æœ¬åœ°æ–‡ä»¶"""
    out_dir = Path("output")
    out_dir.mkdir(exist_ok=True)
    summary = f"# {repo} Issues é€Ÿè§ˆ\n\n{oneliner}\n\n{md_table}"
    (out_dir / "summary.md").write_text(summary, encoding="utf-8")
    (out_dir / "filtered_issues.json").write_text(
        json.dumps(
            [i.model_dump(mode="json") for i in issues],
            ensure_ascii=False,
            indent=2,
            default=str,
        ),
        encoding="utf-8",
    )
    console = Console()
    table = Table(title=f"{repo} é€Ÿè§ˆï¼ˆå‰ 20ï¼‰")
    for col in ["#Issue", "ç±»å‹", "ä¼˜å…ˆçº§", "æ ‡é¢˜"]:
        table.add_column(col, overflow="fold", max_width=30)
    for i in issues[:20]:
        table.add_row(str(i.number), i.type_, i.priority, i.title)
    console.print(table)


# ---------- CLI ----------
app = typer.Typer(help="GitHub Issue é€Ÿè§ˆå™¨ï¼ˆæ”¯æŒ LLM æ‘˜è¦ï¼‰")


@app.command()
def main(
    repo: str = typer.Argument(..., help="owner/repo æ ¼å¼"),
    token: str = typer.Option(None, envvar="GH_TOKEN", help="GitHub Token"),
) -> None:
    """ä¸»å…¥å£"""
    asyncio.run(run(repo, token))


async def run(repo: str, token: str | None) -> None:
    """å¼‚æ­¥ä¸»æµç¨‹"""
    console = Console()
    
    try:
        with console.status("[bold green]æ­£åœ¨æŠ“å– Issues...") as status:
            issues = await fetch_issues(repo, token)
            if not issues:
                console.print("[yellow]âš ï¸ æœªæ‰¾åˆ°ä»»ä½•ç¬¦åˆæ¡ä»¶çš„ Issue[/]")
                return
            
            status.update("[bold green]æ­£åœ¨ç”Ÿæˆæ‘˜è¦...")
            oneliner, md_table = await build_summary_async(issues, repo)
            
            status.update("[bold green]æ­£åœ¨ä¿å­˜ç»“æœ...")
            save_outputs(repo, oneliner, md_table, issues)
            
            console.print("[green]âœ… å®Œæˆï¼ç»“æœå·²ä¿å­˜è‡³ output/ ç›®å½•[/]")
            
    except RepoNotFoundError:
        console.print(f"[red]âŒ ä»“åº“ {repo} ä¸å­˜åœ¨æˆ–æ— æ³•è®¿é—®[/]")
        sys.exit(1)
    except TokenError as e:
        console.print(f"[red]âŒ Token æ— æ•ˆæˆ–æƒé™ä¸è¶³: {e}[/]")
        sys.exit(1)
    except RateLimitError as e:
        reset_time = datetime.fromtimestamp(e.reset_time).strftime("%H:%M:%S")
        console.print(f"[red]âŒ GitHub API é€Ÿç‡é™åˆ¶ï¼Œå°†åœ¨ {reset_time} é‡ç½®[/]")
        sys.exit(1)
    except NetworkError as e:
        console.print(f"[red]âŒ ç½‘ç»œé”™è¯¯: {e}[/]")
        sys.exit(1)
    except Exception as e:
        console.print(f"[red]âŒ æœªçŸ¥é”™è¯¯: {e}[/]")
        logger.exception("Unexpected error")
        sys.exit(1)

if __name__ == "__main__":
    app()
